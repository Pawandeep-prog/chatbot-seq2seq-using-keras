{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:19.463484Z\",\"iopub.execute_input\":\"2021-08-15T01:55:19.463856Z\",\"iopub.status.idle\":\"2021-08-15T01:55:19.469602Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:19.463818Z\",\"shell.execute_reply\":\"2021-08-15T01:55:19.468730Z\"}}\nimport numpy as np \nimport pandas as pd \nimport os\nimport json \nimport tqdm\nimport re\n\n# %% [markdown]\n# # Preprocessing\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:19.493514Z\",\"iopub.execute_input\":\"2021-08-15T01:55:19.493793Z\",\"iopub.status.idle\":\"2021-08-15T01:55:21.568857Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:19.493765Z\",\"shell.execute_reply\":\"2021-08-15T01:55:21.567985Z\"}}\n\ndef clean_text(txt):\n    txt = txt.lower()    \n    txt.replace(\"\\'d\", \" had\")\n    txt.replace(\"\\'s\", \" is\")\n    txt.replace(\"\\'m\", \" am\")\n    txt.replace(\"don't\", \"do not\")\n    txt = re.sub(r'\\W+', ' ', txt)\n    \n    return txt\n\nwith open(\"../input/chatbot/data.json\", \"rb\") as f:\n\ttxt = json.load(f)\n\nchat_in = []\nchat_out = []\nmax_len = 30\n\nfor i in tqdm.tqdm(txt):\n    for index in range(len(i['dialog'])-1):\n        if (len(i['dialog'][index]['text']) < max_len) and (len(i['dialog'][index +1]['text']) < max_len):\n            chat_in.append(\"<sos> \" + clean_text(i['dialog'][index]['text']) + \" <eos>\")\n            chat_out.append(\"<sos> \" + clean_text(i['dialog'][index+1]['text'])+ \" <eos>\")\n\n\nvocabulary = {}\ncount = 1\nfor i in chat_in:\n    for j in i.split(' '):\n        if j not in vocabulary:\n            vocabulary[j] = count\n            count += 1\nfor i in chat_out:\n    for j in i.split(' '):\n        if j not in vocabulary:\n            vocabulary[j] = count\n            count += 1\n\n######## padding \nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nchat_in_encoded = []\nchat_out_encoded = []\nfor i in chat_in:\n    seq = []\n    for j in i.split():\n        seq.append(vocabulary[j])\n    chat_in_encoded.append(pad_sequences([seq], maxlen=max_len+2, padding=\"post\").reshape(-1))\nchat_in_encoded = np.array(chat_in_encoded)\n\n\nfor i in chat_out:\n    seq = []\n    for j in i.split():\n        seq.append(vocabulary[j])\n    chat_out_encoded.append(pad_sequences([seq], maxlen=max_len+2, padding=\"post\").reshape(-1))\nchat_out_encoded = np.array(chat_out_encoded)\n\n\nvocabulary['<pad>'] = 0\n\nfinal_chat_out = []\nfor i in chat_out_encoded:\n    final_chat_out.append(i[1:])\nfinal_chat_out = np.array(final_chat_out)\nfinal_chat_out = pad_sequences(final_chat_out, max_len+2, padding=\"post\")\n\nfrom keras.utils import to_categorical\nfinal_chat_out = to_categorical(final_chat_out, num_classes=len(vocabulary))\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:21.570601Z\",\"iopub.execute_input\":\"2021-08-15T01:55:21.570974Z\",\"iopub.status.idle\":\"2021-08-15T01:55:21.578369Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:21.570944Z\",\"shell.execute_reply\":\"2021-08-15T01:55:21.577297Z\"}}\n# decoder_final_output, decoder_final_input, encoder_final, vocab, inv_vocab\n\nVOCAB_SIZE = len(vocabulary)\nMAX_LEN = max_len+2\nprint(VOCAB_SIZE)\n#print(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:21.579901Z\",\"iopub.execute_input\":\"2021-08-15T01:55:21.580385Z\",\"iopub.status.idle\":\"2021-08-15T01:55:21.588657Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:21.580342Z\",\"shell.execute_reply\":\"2021-08-15T01:55:21.587888Z\"}}\ninv_vocab = {w:k for k,w in vocabulary.items()}\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:21.614026Z\",\"iopub.execute_input\":\"2021-08-15T01:55:21.614535Z\",\"iopub.status.idle\":\"2021-08-15T01:55:21.619600Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:21.614494Z\",\"shell.execute_reply\":\"2021-08-15T01:55:21.618740Z\"}}\nprint(final_chat_out.shape)\n\n# %% [markdown]\n# # Glove Embedding\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:21.621029Z\",\"iopub.execute_input\":\"2021-08-15T01:55:21.621637Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.226376Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:21.621598Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.225407Z\"}}\n\nembeddings_index = {}\nwith open('../input/glove6b50d/glove.6B.50d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\nprint(\"Glove Loded!\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.227675Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.228219Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.232706Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.228177Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.231566Z\"}}\nvocab = vocabulary\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.234529Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.234990Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.249169Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.234950Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.248056Z\"}}\n\nembedding_dimention = 50\ndef embedding_matrix_creater(embedding_dimention, word_index):\n    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\nembedding_matrix = embedding_matrix_creater(50, word_index=vocab)    \n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.250413Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.250790Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.344415Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.250753Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.343177Z\"}}\ndel(embeddings_index)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.369825Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.370439Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.377019Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.370395Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.375622Z\"}}\nembedding_matrix.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.378626Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.379428Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.389322Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.379386Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.388028Z\"}}\nembedding_matrix[0]\n\n# %% [markdown]\n# # Model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.391242Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.392028Z\",\"iopub.status.idle\":\"2021-08-15T01:55:31.401530Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.391988Z\",\"shell.execute_reply\":\"2021-08-15T01:55:31.400381Z\"}}\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:31.404255Z\",\"iopub.execute_input\":\"2021-08-15T01:55:31.404859Z\",\"iopub.status.idle\":\"2021-08-15T01:55:33.405400Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:31.404822Z\",\"shell.execute_reply\":\"2021-08-15T01:55:33.404611Z\"}}\nembed = Embedding(VOCAB_SIZE+1, \n                  50,\n                  input_length=MAX_LEN,\n                  trainable=True)\n\nembed.build((None,))\nembed.set_weights([embedding_matrix])\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:33.407625Z\",\"iopub.execute_input\":\"2021-08-15T01:55:33.407910Z\",\"iopub.status.idle\":\"2021-08-15T01:55:33.417859Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:33.407883Z\",\"shell.execute_reply\":\"2021-08-15T01:55:33.416941Z\"}}\nenc_inp = Input(shape=(MAX_LEN,))\ndec_inp = Input(shape=(MAX_LEN,))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:33.419638Z\",\"iopub.execute_input\":\"2021-08-15T01:55:33.420009Z\",\"iopub.status.idle\":\"2021-08-15T01:55:34.466354Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:33.419972Z\",\"shell.execute_reply\":\"2021-08-15T01:55:34.465597Z\"}}\nenc_embed = embed(enc_inp)\n\nenc_lstm = Bidirectional(LSTM(200, return_sequences=True, return_state=True))\nenc_op, f_h, f_c, b_h, b_c = enc_lstm(enc_embed)\n\nh = Concatenate()([f_h, b_h])\nc = Concatenate()([f_c, b_c])\nenc_states = [h, c]\n\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400, return_sequences=True, return_state=True)\ndec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\ndense = Dense(VOCAB_SIZE, activation='softmax')\n\ndense_op = dense(dec_op)\n\nmodel = Model([enc_inp, dec_inp], dense_op)\n\nmodel.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')\n\n\n# %% [markdown]\n# # Adding CallBack\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:34.467580Z\",\"iopub.execute_input\":\"2021-08-15T01:55:34.467951Z\",\"iopub.status.idle\":\"2021-08-15T01:55:34.475155Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:34.467913Z\",\"shell.execute_reply\":\"2021-08-15T01:55:34.474245Z\"}}\nfrom tensorflow.keras.callbacks import Callback\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:34.476589Z\",\"iopub.execute_input\":\"2021-08-15T01:55:34.477176Z\",\"iopub.status.idle\":\"2021-08-15T01:55:34.496824Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:34.477135Z\",\"shell.execute_reply\":\"2021-08-15T01:55:34.495889Z\"}}\nclass PrintCallBack(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 10 == 0:\n            enc_model = Model([enc_inp], enc_states)\n\n            # decoder Model\n            decoder_state_input_h = Input(shape=(400,))\n            decoder_state_input_c = Input(shape=(400,))\n\n            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\n            decoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n                                                initial_state=decoder_states_inputs)\n\n\n            decoder_states = [state_h, state_c]\n\n\n            dec_model = Model([dec_inp, decoder_states_inputs],\n                                                  [decoder_outputs, decoder_states])\n            \n\n            ins = [\"hello\", \"how are you\", \"what is your name\", \"i want to marry you\", \"i will kill you\", \"i hate you\", \"what do you do\"]\n            \n            for i in ins:\n                print()\n                print(i)\n                \n                prepro1 = i\n                prepro1 = clean_text(prepro1)\n                ## prepro1 = \"hello\"\n                prepro1 = \"<sos> \" + prepro1 + \" <eos>\"\n\n                prepro = [prepro1]\n                ## prepro1 = [\"hello\"]\n\n                txt = []\n                for x in prepro:\n                    # x = \"hello\"\n                    lst = []\n                    for y in x.split():\n                        ## y = \"hello\"\n                        try:\n                            lst.append(vocabulary[y])\n                            ## vocab['hello'] = 454\n                        except:\n                            lst.append(vocabulary['<OUT>'])\n                    txt.append(lst)\n\n                ## txt = [[454]]\n                txt = pad_sequences(txt, max_len+2, padding='post')\n\n                ## txt = [[454,0,0,0,.........13]]\n\n                stat = enc_model.predict( txt )\n\n                empty_target_seq = np.zeros( ( 1 , 1) )\n                 ##   empty_target_seq = [0]\n\n\n                empty_target_seq[0, 0] = vocabulary['<sos>']\n                ##    empty_target_seq = [255]\n\n                stop_condition = False\n                decoded_translation = ''\n\n                while not stop_condition :\n\n                    dec_outputs , dec_states_op= dec_model.predict([ empty_target_seq, stat] )\n                    decoder_concat_input = dense(dec_outputs)\n                    ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n\n                    sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n                    ## sampled_word_index = [2]\n\n                    sampled_word = inv_vocab[sampled_word_index] + ' '\n\n                    ## inv_vocab[2] = 'hi'\n                    ## sampled_word = 'hi '\n\n                    if sampled_word != '<eos> ':\n                        decoded_translation += sampled_word  \n\n                    if sampled_word == '<eos> ' or len(decoded_translation.split()) > max_len:\n                        stop_condition = True \n\n                    empty_target_seq = np.zeros( ( 1 , 1 ) )  \n                    empty_target_seq[ 0 , 0 ] = sampled_word_index\n                    ## <SOS> - > hi\n                    ## hi --> <EOS>\n                    stat = dec_states_op.copy()\n\n                print(\"chatbot attention : \", decoded_translation )\n                print(\"==============================================\")  \n                        \n\n\n# %% [markdown]\n# # Fitting the model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T01:55:34.498174Z\",\"iopub.execute_input\":\"2021-08-15T01:55:34.498582Z\",\"iopub.status.idle\":\"2021-08-15T02:11:48.631829Z\",\"shell.execute_reply.started\":\"2021-08-15T01:55:34.498542Z\",\"shell.execute_reply\":\"2021-08-15T02:11:48.629259Z\"}}\n\n\nmodel.fit([chat_in_encoded, chat_out_encoded], final_chat_out, epochs = 400, batch_size=24, callbacks=[PrintCallBack()])\n\n\n#######################################################################\n\n\n# %% [markdown]\n# # Inference Setup\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T02:11:55.595574Z\",\"iopub.execute_input\":\"2021-08-15T02:11:55.595936Z\",\"iopub.status.idle\":\"2021-08-15T02:11:55.798344Z\",\"shell.execute_reply.started\":\"2021-08-15T02:11:55.595900Z\",\"shell.execute_reply\":\"2021-08-15T02:11:55.797610Z\"}}\nenc_model = Model([enc_inp], enc_states)\n\n\n\n# decoder Model\ndecoder_state_input_h = Input(shape=(400,))\ndecoder_state_input_c = Input(shape=(400,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n                                    initial_state=decoder_states_inputs)\n\n\ndecoder_states = [state_h, state_c]\n\n\ndec_model = Model([dec_inp, decoder_states_inputs],\n                                      [decoder_outputs, decoder_states])\n\n\n\n\n\n\n# %% [markdown]\n# # Inference Part\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-08-15T02:23:19.000930Z\",\"iopub.execute_input\":\"2021-08-15T02:23:19.001257Z\"}}\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    prepro1  = input(\"you : \")\n    ## prepro1 = \"Hello\"\n    \n    \n    prepro1 = clean_text(prepro1)\n    ## prepro1 = \"hello\"\n    prepro1 = \"<sos> \" + prepro1 + \" <eos>\"\n\n    prepro = [prepro1]\n    ## prepro1 = [\"hello\"]\n\n    txt = []\n    for x in prepro:\n        # x = \"hello\"\n        lst = []\n        for y in x.split():\n            ## y = \"hello\"\n            try:\n                lst.append(vocabulary[y])\n                ## vocab['hello'] = 454\n            except:\n                lst.append(vocabulary['<OUT>'])\n        txt.append(lst)\n\n    ## txt = [[454]]\n    txt = pad_sequences(txt, max_len+2, padding='post')\n\n    ## txt = [[454,0,0,0,.........13]]\n\n    stat = enc_model.predict( txt )\n\n    empty_target_seq = np.zeros( ( 1 , 1) )\n     ##   empty_target_seq = [0]\n\n\n    empty_target_seq[0, 0] = vocabulary['<sos>']\n    ##    empty_target_seq = [255]\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition :\n\n        dec_outputs , dec_states_op= dec_model.predict([ empty_target_seq, stat] )\n        decoder_concat_input = dense(dec_outputs)\n        ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n        ## sampled_word_index = [2]\n\n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        ## inv_vocab[2] = 'hi'\n        ## sampled_word = 'hi '\n\n        if sampled_word != '<eos> ':\n            decoded_translation += sampled_word  \n\n        if sampled_word == '<eos> ' or len(decoded_translation.split()) > max_len:\n            stop_condition = True \n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        ## <SOS> - > hi\n        ## hi --> <EOS>\n        stat = dec_states_op.copy()\n\n    print(\"chatbot : \", decoded_translation )\n    print(\"==============================================\")  \n\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n","metadata":{"_uuid":"a8bfef16-1788-47fa-8da7-6301ca0d5d1f","_cell_guid":"4ab89304-92e3-4d6f-baf6-f65fe958acd6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}